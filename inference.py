import warnings
from collections import deque

from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import ccxt
import joblib
import numpy as np
import pandas as pd
import ta
import torch
from lightning.pytorch import LightningModule
from pykalman import KalmanFilter
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from torch import nn
from stable_baselines3 import PPO
import gymnasium as gym
import torch.nn.functional as F
import os
import time
import json
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
import io
from datetime import datetime
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
import glob
from google.oauth2.credentials import Credentials
from googleapiclient.errors import HttpError
import re

creds_data = json.loads(os.environ['GOOGLE_OAUTH_CREDENTIALS'])
creds = Credentials(
    token=creds_data.get("token"),
    refresh_token=creds_data.get("refresh_token"),
    token_uri=creds_data.get("token_uri"),
    client_id=creds_data.get("client_id"),
    client_secret=creds_data.get("client_secret"),
    scopes=creds_data.get("scopes"),
)

service = build('drive', 'v3', credentials=creds)

LOG_FOLDER_ID = "12WcA1K7_wR8eujJr7aGzMs_GJMfPPpYK"
LOCAL_LOG_DIR = "logs"
LOG_PATTERN = r"btc_rl_tail_.*\.json"

def get_latest_tail_log_json(service, folder_id=LOG_FOLDER_ID, local_dir=LOCAL_LOG_DIR, pattern=LOG_PATTERN):
    query = f"'{folder_id}' in parents and mimeType='application/json' and name contains 'btc_rl_tail_'"
    try:
        results = service.files().list(q=query,
                                       spaces='drive',
                                       fields="files(id, name, createdTime)",
                                       orderBy="createdTime desc").execute()
        items = results.get('files', [])
        if not items:
            print("–ù–µ—Ç json-—Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ Google Drive.")
            return None

        for f in items:
            if re.fullmatch(pattern, f["name"]):
                file_id = f["id"]
                file_name = f["name"]
                break
        else:
            print("–ù–µ—Ç —Ñ–∞–π–ª–∞, –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø–æ–¥ –ø–∞—Ç—Ç–µ—Ä–Ω.")
            return None

        os.makedirs(local_dir, exist_ok=True)
        local_path = os.path.join(local_dir, file_name)

        request = service.files().get_media(fileId=file_id)
        fh = io.FileIO(local_path, 'wb')
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()

        print(f"[‚úì] Json-—Ñ–∞–π–ª —Å–∫–∞—á–∞–Ω –ª–æ–∫–∞–ª—å–Ω–æ: {local_path}")
        return local_path

    except HttpError as e:
        print(f"–û—à–∏–±–∫–∞ Google Drive API: {e}")
        return None

def update_log_json_and_upload(service, new_entry: dict, local_dir=LOCAL_LOG_DIR, folder_id=LOG_FOLDER_ID):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π json –ª–æ–≥,
    –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ new_entry),
    –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å (–µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ),
    —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª –æ–±—Ä–∞—Ç–Ω–æ.
    """
    os.makedirs(local_dir, exist_ok=True)
    local_path = get_latest_tail_log_json(service, folder_id, local_dir)
    log_data = []

    if local_path and os.path.exists(local_path):
        with open(local_path, "r", encoding="utf-8") as f:
            try:
                log_data = json.load(f)
            except json.JSONDecodeError:
                print("[!] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è json, –ª–æ–≥ —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø—É—Å—Ç—ã–º.")
                log_data = []

    # –ï—Å–ª–∏ –≤ new_entry –µ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å
    real_data_present = new_entry.get("real_price") is not None or new_entry.get("real_class") is not None
    if real_data_present and log_data:
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å
        last_entry = log_data[-1]
        if new_entry.get("real_price") is not None:
            last_entry["real_price"] = new_entry["real_price"]
        if new_entry.get("real_class") is not None:
            last_entry["real_class"] = new_entry["real_class"]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–ø–∏—Å–∏
        log_data[-1] = last_entry
        print("[‚úì] –û–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–ø–∏—Å—å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.")

    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–æ–±—ã—á–Ω–æ real_price –∏ real_class –ø—É—Å—Ç—ã–µ)
    if not real_data_present:
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
        log_data.append(new_entry)
        print("[‚úì] –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º
    timestamp_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    new_file_name = f"btc_rl_tail_{timestamp_str}.json"
    new_local_path = os.path.join(local_dir, new_file_name)

    with open(new_local_path, "w", encoding="utf-8") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ Google Drive
    file_metadata = {
        "name": new_file_name,
        "parents": [folder_id],
        "mimeType": "application/json"
    }
    media = MediaFileUpload(new_local_path, mimetype="application/json")

    # –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É —Ñ–∞–π–ª–∞ –∫–∞–∫ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ
    service.files().create(body=file_metadata, media_body=media, fields="id").execute()
    print(f"[‚úì] –õ–æ–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ Google Drive: {new_file_name}")


def get_latest_tail_log(folder_id="12WcA1K7_wR8eujJr7aGzMs_GJMfPPpYK", local_dir="logs", pattern=r"btc_rl_tail_.*\.csv"):
    # –ó–∞–ø—Ä–æ—Å –∫ Google Drive ‚Äî –ø–æ–ª—É—á–∏—Ç—å –≤—Å–µ csv-—Ñ–∞–π–ª—ã —Å –Ω—É–∂–Ω–æ–π –º–∞—Å–∫–æ–π –≤ –ø–∞–ø–∫–µ
    query = f"'{folder_id}' in parents and mimeType='text/csv' and name contains 'btc_rl_tail_'"
    try:
        results = service.files().list(q=query,
                                       spaces='drive',
                                       fields="files(id, name, createdTime)",
                                       orderBy="createdTime desc").execute()
        items = results.get('files', [])
        if not items:
            raise FileNotFoundError("–í —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ Google Drive –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ —Å –Ω—É–∂–Ω–æ–π –º–∞—Å–∫–æ–π.")

        # –ù–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É
        for f in items:
            if re.fullmatch(pattern, f["name"]):
                file_id = f["id"]
                file_name = f["name"]
                break
        else:
            raise FileNotFoundError("–ù–µ—Ç —Ñ–∞–π–ª–∞, –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø–æ–¥ –ø–∞—Ç—Ç–µ—Ä–Ω.")

        # –°–æ–∑–¥–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É, –µ—Å–ª–∏ –Ω–µ—Ç
        os.makedirs(local_dir, exist_ok=True)
        local_path = os.path.join(local_dir, file_name)

        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
        request = service.files().get_media(fileId=file_id)
        fh = io.FileIO(local_path, 'wb')
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()

        print(f"[‚úì] –§–∞–π–ª —Å–∫–∞—á–∞–Ω –ª–æ–∫–∞–ª—å–Ω–æ: {local_path}")
        return local_path

    except HttpError as e:
        print(f"–û—à–∏–±–∫–∞ Google Drive API: {e}")
        raise


def download_csv_old(file_id, local_path='temp.csv'):
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(local_path, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while done is False:
        status, done = downloader.next_chunk()
    fh.close()
    return local_path


def upload_file_by_id(file_id, filename, mimetype='text/csv'):
    media = MediaFileUpload(filename, mimetype=mimetype)
    service.files().update(
        fileId=file_id,
        media_body=media
    ).execute()
    print(f"[‚Üª] –§–∞–π–ª {filename} –æ–±–Ω–æ–≤–ª—ë–Ω –ø–æ ID: {file_id}")

upload_file_by_id('1aca3KCgbVKzAqGrNMRzcI5NClfBM5QYM', 'btc_model_predictions.csv')

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
FOLDER_ID = "12WcA1K7_wR8eujJr7aGzMs_GJMfPPpYK"
TAIL_FILENAME_PREFIX = "btc_rl_tail"
LOCAL_TEMP_CSV = "btc_rl_tail_temp.csv"

# === –§—É–Ω–∫—Ü–∏—è: –ø–æ–ª—É—á–∏—Ç—å ID –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ (–ø–æ –≤—Ä–µ–º–µ–Ω–∏) —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫–µ ===
def get_latest_file_id_in_folder(folder_id):
    query = f"'{folder_id}' in parents and mimeType='text/csv'"
    results = service.files().list(q=query, orderBy="createdTime desc", pageSize=1).execute()
    items = results.get("files", [])
    if not items:
        return None
    return items[0]["id"]

# === –§—É–Ω–∫—Ü–∏—è: —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø–æ ID ===
def download_csv(file_id, local_path):
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(local_path, "wb")
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    fh.close()

# === –§—É–Ω–∫—Ü–∏—è: –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É —Å –Ω–æ–≤—ã–º –∏–º–µ–Ω–µ–º ===
def upload_csv_to_folder(local_path, folder_id):
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
    filename = f"{TAIL_FILENAME_PREFIX}_{timestamp}.csv"
    file_metadata = {
        "name": filename,
        "parents": [folder_id]
    }
    media = MediaFileUpload(local_path, mimetype="text/csv")
    file = service.files().create(body=file_metadata, media_body=media, fields="id").execute()
    print(f"[‚Üë] –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {filename} ‚Üí ID: {file.get('id')}")

def get_prediction_report():
    def get_btc_and_append_csv(filename='btc_data_15m.csv', return_days=7):
        # exchange = ccxt.binance()
        exchange = ccxt.okx()
        symbol = 'BTC/USDT'
        timeframe = '15m'
        limit = 1000

        now = exchange.milliseconds()
        seven_days_ago = now - 7 * 24 * 60 * 60 * 1000  # 7 days in ms

        download_csv_old('1ydQ_MmeqGNBvqVNWpMjIA4YXl1J9N6hq', 'btc_data_15m.csv')

        # –ü—Ä–æ—á–∏—Ç–∞—Ç—å
        btc_data_15m = pd.read_csv('btc_data_15m.csv')
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º 7 –¥–Ω–µ–π
        if btc_data_15m.empty:
            print("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ 7 –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö...")
            ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=seven_days_ago, limit=limit)
            df = pd.DataFrame(ohlcv, columns=['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])
            df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='ms')
            df.set_index('Timestamp', inplace=True)
            df.to_csv(filename)
        else:
            existing_df = pd.read_csv(filename, parse_dates=['Timestamp'], index_col='Timestamp')
            last_timestamp_local = existing_df.index.max()

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–≤–µ—á—É —Å –±–∏—Ä–∂–∏
            ohlcv_latest = exchange.fetch_ohlcv(symbol, timeframe=timeframe, limit=1)
            last_timestamp_remote = pd.to_datetime(ohlcv_latest[-1][0], unit='ms')

            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç ‚Äî –∂–¥—ë–º 30 —Å–µ–∫ –∏ –ø—Ä–æ–±—É–µ–º –∑–∞–Ω–æ–≤–æ
            if last_timestamp_local >= last_timestamp_remote:
                print(f"[‚è≥] –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–≤–µ—á–∞ ({last_timestamp_local}) —É–∂–µ –≤ —Ñ–∞–π–ª–µ. –ñ–¥—ë–º 30 —Å–µ–∫—É–Ω–¥...")
                time.sleep(30)
                return get_btc_and_append_csv(filename, return_days=return_days)

            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å {last_timestamp_local + pd.Timedelta(minutes=15)}")
            last_timestamp_ms = int(last_timestamp_local.timestamp() * 1000) + 1
            fetch_since = last_timestamp_ms

            max_candles = 1000
            all_new_data = []

            while fetch_since < now:
                try:
                    ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=fetch_since, limit=max_candles)
                    if not ohlcv:
                        break
                    df_new = pd.DataFrame(ohlcv, columns=['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])
                    df_new['Timestamp'] = pd.to_datetime(df_new['Timestamp'], unit='ms')
                    df_new.set_index('Timestamp', inplace=True)

                    df_new = df_new[~df_new.index.isin(existing_df.index)]
                    all_new_data.append(df_new)

                    fetch_since = int(df_new.index[-1].timestamp() * 1000) + 1
                    time.sleep(0.2)
                except Exception as e:
                    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö:", e)
                    break

            if all_new_data:
                full_new_data = pd.concat(all_new_data)
                updated_df = pd.concat([existing_df, full_new_data])
                updated_df = updated_df[~updated_df.index.duplicated(keep='last')]
                updated_df.sort_index(inplace=True)
                updated_df.to_csv(filename)
            else:
                updated_df = existing_df

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ return_days —Å—É—Ç–æ–∫
        updated_df = pd.read_csv(filename, parse_dates=['Timestamp'], index_col='Timestamp')
        cutoff = updated_df.index.max() - pd.Timedelta(days=return_days)
        filtered_df = updated_df[updated_df.index >= cutoff]
        upload_file_by_id('1ydQ_MmeqGNBvqVNWpMjIA4YXl1J9N6hq','btc_data_15m.csv')
        return filtered_df

    class TransformerBinaryClassifier(LightningModule):
        def __init__(self, input_size, d_model=64, nhead=4, num_layers=2, lr=1e-3):
            super().__init__()
            self.save_hyperparameters()

            self.embedding = nn.Linear(input_size, d_model)
            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
            self.fc_out = nn.Linear(d_model, 1)
            self.lr = lr

        def forward(self, x):
            # x: (batch, seq_len, features)
            x = self.embedding(x)
            x = x.permute(1, 0, 2)  # (seq_len, batch, d_model)
            x = self.transformer(x)
            out = self.fc_out(x[-1])  # –±–µ—Ä–µ–º –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —à–∞–≥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            return out.squeeze(-1)

        def training_step(self, batch, batch_idx):
            x, y, _ = batch
            x = x.float()  # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
            y_hat = self(x)
            loss = F.binary_cross_entropy_with_logits(y_hat, y)
            self.log("train_loss", loss)
            return loss

        def validation_step(self, batch, batch_idx):
            x, y, _ = batch
            x = x.float()
            y_hat = self(x)
            loss = F.binary_cross_entropy_with_logits(y_hat, y)
            self.log("val_loss", loss)
            return loss

        def test_step(self, batch, batch_idx):
            x, y, _ = batch
            x = x.float()
            y_hat = self(x)
            loss = F.binary_cross_entropy_with_logits(y_hat, y)
            self.log("test_loss", loss)
            # –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            return loss

        def configure_optimizers(self):
            return torch.optim.Adam(self.parameters(), lr=self.lr)

    class BTCTradingEnv(gym.Env):
        def __init__(self, df, state_columns,
                     initial_balance=10_000,
                     trade_penalty=0.01,
                     max_steps=None,
                     reward_scaling=100.0,
                     use_log_return=False,
                     use_sharpe_bonus=False,
                     holding_penalty=0.005,
                     sharpe_bonus_weight=0.5,
                     commission=0.001,
                     spread=0.0005,
                     slippage_std=0.001,
                     min_holding_period=8,
                     window_size=672):

            super().__init__()
            self.df = df.reset_index(drop=True)
            self.state_columns = state_columns
            self.window_size = window_size

            self.commission = commission
            self.spread = spread
            self.slippage_std = slippage_std
            self.min_holding_period = min_holding_period

            self.action_space = gym.spaces.Discrete(3)  # 0: hold, 1: buy, 2: sell
            self.observation_space = gym.spaces.Box(
                low=-np.inf, high=np.inf,
                shape=(window_size, len(state_columns)),
                dtype=np.float32
            )

            self.initial_balance = initial_balance
            self.trade_penalty = trade_penalty
            self.reward_scaling = reward_scaling
            self.max_steps = max_steps if max_steps is not None else len(df) - 1

            self.use_log_return = use_log_return
            self.use_sharpe_bonus = use_sharpe_bonus
            self.holding_penalty = holding_penalty
            self.sharpe_bonus_weight = sharpe_bonus_weight

            self.all_trades_info = []
            self.episode_trades_info = []

            self.reset()

        def _get_execution_price(self, price, action):
            if action == 1:
                exec_price = price * (1 + self.spread)
            elif action == 2:
                exec_price = price * (1 - self.spread)
            else:
                exec_price = price

            slippage = np.random.normal(0, self.slippage_std)
            exec_price *= (1 + slippage)
            return max(exec_price, 0.0001)

        def _next_observation(self):
            obs = self.df.iloc[self.current_step][self.state_columns].astype(np.float32).values
            obs = np.nan_to_num(obs)
            self.state_window.append(obs)
            return np.array(self.state_window)

        def _calculate_equity(self, current_price):
            if self.position == 1:
                return self.balance + (current_price - self.entry_price)
            return self.balance

        def _calculate_max_drawdown(self):
            equity = np.array(self.equity_curve)
            if len(equity) < 2:
                return 0
            cumulative_max = np.maximum.accumulate(equity)
            drawdowns = (equity - cumulative_max) / cumulative_max
            return drawdowns.min()

        def reset(self, *, seed=None, options=None):
            self.current_step = self.window_size
            self.balance = self.initial_balance
            self.position = 0
            self.entry_price = 0.0
            self.entry_step = None
            self.total_reward = 0.0
            self.trades = []
            self.trades_info = []
            self.equity_curve = [self.balance]
            self.episode_trades_info = []
            self.actions_log = []
            self.state_window = deque(maxlen=self.window_size)

            for i in range(self.current_step - self.window_size, self.current_step):
                obs = self.df.iloc[i][self.state_columns].astype(np.float32).values
                self.state_window.append(np.nan_to_num(obs))

            return self._next_observation(), {}

        def step(self, action):
            done = False
            reward = 0.0

            market_price = self.df.loc[self.current_step, 'Close']
            if np.isnan(market_price) or market_price <= 0:
                market_price = 1.0

            max_holding_period = 96
            if self.position == 1 and (self.current_step - self.entry_step) >= max_holding_period:
                action = 2  # —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞

            exec_price = self._get_execution_price(market_price, action)

            # --- BUY ---
            if action == 1 and self.position == 0:
                commission_cost = exec_price * self.commission
                reward += 0.1
                self.position = 1
                self.entry_price = exec_price + commission_cost
                self.entry_step = self.current_step
                self.balance -= commission_cost

            # --- SELL ---
            elif action == 2 and self.position == 1:
                commission_cost = exec_price * self.commission
                price_change = (exec_price - self.entry_price) / self.entry_price
                holding_duration = self.current_step - self.entry_step
                reward = price_change - self.commission * 2

                if holding_duration < self.min_holding_period:
                    reward -= 0.1
                if reward > 0:
                    reward += 0.2

                self.balance += price_change * self.entry_price
                self.balance -= commission_cost

                trade_info = {
                    "entry_step": self.entry_step,
                    "exit_step": self.current_step,
                    "profit": price_change - self.commission * 2,
                    "holding": holding_duration,
                    "entry_price": self.entry_price,
                    "exit_price": exec_price
                }
                self.trades.append(price_change - self.commission * 2)
                self.episode_trades_info.append(trade_info)
                self.all_trades_info.append(trade_info)

                self.position = 0
                self.entry_price = 0.0
                self.entry_step = None

            # --- Bad actions ---
            elif action == 2 and self.position == 0:
                reward -= 0.005
            elif action == 1 and self.position == 1:
                reward -= 0.005

            # --- HOLD ---
            elif action == 0:
                if self.position == 1:
                    # –ù–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–∏–±—ã–ª—å (–Ω–∞–≥—Ä–∞–∂–¥–∞–µ–º –∑–∞ –≤—ã–≥–æ–¥–Ω—ã–µ —É–¥–µ—Ä–∂–∞–Ω–∏—è)
                    unrealized = (market_price - self.entry_price) / self.entry_price
                    reward += unrealized * 0.1

                    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–∏–π —Ö–æ–ª–¥
                    holding_duration = self.current_step - self.entry_step
                    if holding_duration > self.min_holding_period:
                        reward -= self.holding_penalty * max(0, holding_duration - self.min_holding_period)
                else:
                    # –ù–∞—Ö–æ–¥–∏–º—Å—è –≤–Ω–µ –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–æ—Å—Ç–æ —Ö–æ–ª–¥–∏–º ‚Äî —à—Ç—Ä–∞—Ñ—É–µ–º —Å–ª–µ–≥–∫–∞
                    reward -= 0.001

            reward = np.clip(reward, -100.0, 100.0)
            if np.isnan(reward) or np.isinf(reward):
                reward = 0.0

            self.total_reward += reward
            equity = self._calculate_equity(market_price)
            self.equity_curve.append(equity)

            self.current_step += 1
            done = self.current_step >= self.max_steps

            # --- Force close if still holding ---
            if done and self.position == 1:
                exec_price = self._get_execution_price(market_price, 2)
                commission_cost = exec_price * self.commission
                final_price_change = (exec_price - self.entry_price) / self.entry_price - self.commission * 2
                final_reward = final_price_change

                self.balance += final_price_change * self.entry_price
                self.balance -= commission_cost

                trade_info = {
                    "entry_step": self.entry_step,
                    "exit_step": self.current_step,
                    "profit": final_price_change,
                    "holding": self.current_step - self.entry_step,
                    "entry_price": self.entry_price,
                    "exit_price": exec_price
                }
                self.trades.append(final_price_change)
                self.episode_trades_info.append(trade_info)
                self.all_trades_info.append(trade_info)
                self.total_reward += final_reward * self.reward_scaling
                self.position = 0

            # --- Sharpe Bonus ---
            if done and self.use_sharpe_bonus and len(self.trades) > 1:
                sharpe = np.mean(self.trades) / (np.std(self.trades) + 1e-8)
                reward += self.sharpe_bonus_weight * sharpe

            self.actions_log.append(action)

            obs = self._next_observation()
            info = {
                'balance': self.balance,
                'equity': equity,
                'position': self.position,
                'step': self.current_step,
                'reward': reward,
                'drawdown': self._calculate_max_drawdown(),
                'total_profit': np.sum(self.trades),
                'recent_trade': self.episode_trades_info[-1] if self.episode_trades_info else None,
                'combined_signal': self.df.loc[
                    self.current_step, 'combined_signal'] if 'combined_signal' in self.df.columns else None,
                'rsi': self.df.loc[self.current_step, 'rsi'] if 'rsi' in self.df.columns else None,
                'macd': self.df.loc[self.current_step, 'macd'] if 'macd' in self.df.columns else None,
                'volume_spike': self.df.loc[
                    self.current_step, 'volume_spike'] if 'volume_spike' in self.df.columns else None,
                'candle_cluster': self.df.loc[
                    self.current_step, 'candle_cluster'] if 'candle_cluster' in self.df.columns else None,
                'entry_price': self.entry_price if self.position == 1 else None,
                'unrealized_profit': (
                            (market_price - self.entry_price) / self.entry_price) if self.position == 1 else 0.0,
                'holding_duration': (self.current_step - self.entry_step) if self.position == 1 else 0,
                'actions_log': self.actions_log[-100:]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
            }

            return obs, reward, done, False, info

    class TransformerFeatureExtractor(BaseFeaturesExtractor):
        def __init__(self, observation_space, d_model=64, nhead=4, num_layers=2, dropout=0.1):
            # –û–ø—Ä–µ–¥–µ–ª–∏–º —Ä–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–∞
            seq_len, feature_dim = observation_space.shape  # (window_size, num_features)

            super().__init__(observation_space, features_dim=d_model)

            self.seq_len = seq_len
            self.feature_dim = feature_dim

            # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ d_model
            self.input_proj = nn.Linear(feature_dim, d_model)

            self.norm = nn.LayerNorm(d_model)

            # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-—ç–Ω–∫–æ–¥–µ—Ä
            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)
            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω)
            self.pool = nn.AdaptiveAvgPool1d(1)

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            # x: (batch_size, seq_len, feature_dim)
            x = self.input_proj(x)  # -> (batch_size, seq_len, d_model)
            x = self.norm(x)  # üëà –ø–æ—Å–ª–µ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è
            x = self.transformer_encoder(x)  # -> (batch_size, seq_len, d_model)

            # –ë–µ—Ä—ë–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å–∏: (batch_size, d_model)
            x = x.mean(dim=1)
            return x

    class TransformerPolicy(ActorCriticPolicy):
        def __init__(self, observation_space, action_space, lr_schedule,
                     net_arch=None, activation_fn=nn.Tanh, **kwargs):
            # –ó–∞–º–µ–Ω–∏–º feature_extractor –Ω–∞ –Ω–∞—à TransformerFeatureExtractor
            super().__init__(
                observation_space,
                action_space,
                lr_schedule,
                features_extractor_class=TransformerFeatureExtractor,
                features_extractor_kwargs=dict(d_model=64, nhead=4, num_layers=2),
                net_arch=[dict(pi=[64], vf=[64])],
                activation_fn=activation_fn,
                **kwargs
            )
    def compute_indicators_v6(df, trend_emd=None, future_horizon=5, threshold=0.02, kalman_smooth=False):
        if kalman_smooth:

            kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1)
            df = df.copy()

            for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
                smoothed, _ = kf.smooth(df[col].values)
                df[col] = smoothed.flatten()

        price_orig = df['Close']

        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Ç—Ä–µ–Ω–¥ –∏–∑ EMD ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –∫–∞–∫ –æ—á–∏—â–µ–Ω–Ω—É—é —Ü–µ–Ω—É –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (–¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —à—É–º–∞)
        price = trend_emd if trend_emd is not None else price_orig

        o, h, l, c, v = df['Open'], df['High'], df['Low'], df['Close'], df['Volume']

        # --- –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ ---
        df['sma_1d'] = price
        df['sma_1w'] = price.rolling(7).mean()
        df['sma_signal'] = (df['sma_1d'] > df['sma_1w']).astype(int)

        ema12 = price.ewm(span=12, adjust=False).mean()
        ema26 = price.ewm(span=26, adjust=False).mean()
        df['ema_crossover'] = (ema12 > ema26).astype(int)

        # --- RSI ---
        delta = price.diff()
        gain = delta.clip(lower=0)
        loss = -delta.clip(upper=0)
        rs = gain.rolling(14).mean() / (loss.rolling(14).mean() + 1e-8)
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi_signal'] = (df['rsi'] < 30).astype(int)

        # --- MACD ---
        macd = ema12 - ema26
        macd_signal = macd.ewm(span=9, adjust=False).mean()
        df['macd'] = macd
        df['macd_signal'] = macd_signal
        df['macd_signal_bin'] = (macd > macd_signal).astype(int)

        # --- Volatility ---
        window_size = 7
        df['volatility_1d'] = price.rolling(window=window_size).std()
        median_vol = df['volatility_1d'].median()
        df['volatility_signal'] = (df['volatility_1d'] > median_vol).astype(int)
        vol_roll = df['volatility_1d'].rolling(14)
        df['volatility_z'] = (df['volatility_1d'] - vol_roll.mean()) / (vol_roll.std() + 1e-8)

        # --- Bollinger Bands ---
        bb = ta.volatility.BollingerBands(close=price, window=20, window_dev=2)
        df['bb_hband_indicator'] = bb.bollinger_hband_indicator()
        df['bb_lband_indicator'] = bb.bollinger_lband_indicator()

        # --- ATR ---
        df['atr'] = ta.volatility.AverageTrueRange(high=h, low=l, close=c, window=14).average_true_range()

        # --- On Balance Volume ---
        df['obv'] = ta.volume.OnBalanceVolumeIndicator(close=c, volume=v).on_balance_volume()

        # --- Stochastic RSI ---
        df['stoch_rsi'] = ta.momentum.StochasticOscillator(high=h, low=l, close=c, window=14).stoch()

        # --- –ù–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã ---

        # ADX - —Å–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞
        df['adx'] = ta.trend.ADXIndicator(high=h, low=l, close=c, window=14).adx()

        # CCI - –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å/–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å
        df['cci'] = ta.trend.CCIIndicator(high=h, low=l, close=c, window=20).cci()

        # Williams %R
        df['williams_r'] = ta.momentum.WilliamsRIndicator(high=h, low=l, close=c, lbp=14).williams_r()

        # Parabolic SAR
        df['psar'] = ta.trend.PSARIndicator(high=h, low=l, close=c, step=0.02, max_step=0.2).psar()

        # Momentum
        df['momentum'] = c - c.shift(10)

        # Chaikin Money Flow
        df['cmf'] = ta.volume.ChaikinMoneyFlowIndicator(high=h, low=l, close=c, volume=v, window=20).chaikin_money_flow()

        # --- –°–≤–µ—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã ---
        df['bull_candle'] = (c > o).astype(int)
        df['bear_candle'] = (c < o).astype(int)
        hl_range = h - l + 1e-8
        df['hammer'] = ((h - l > 3 * abs(o - c)) &
                        ((c - l) / hl_range > 0.6) &
                        ((o - l) / hl_range > 0.6)).astype(int)
        df['doji'] = (abs(c - o) <= 0.05 * hl_range).astype(int)
        df['shooting_star'] = ((h - l > 3 * abs(o - c)) &
                               ((h - c) / hl_range > 0.6) &
                               ((h - o) / hl_range > 0.6)).astype(int)

        prev_c, prev_o = c.shift(1), o.shift(1)
        df['bullish_engulfing'] = ((prev_c < prev_o) & (c > o) & (c > prev_o) & (o < prev_c)).astype(int)
        df['bearish_engulfing'] = ((prev_c > prev_o) & (c < o) & (c < prev_o) & (o > prev_c)).astype(int)
        df['morning_star'] = ((df['bear_candle'].shift(2) == 1) &
                              (df['doji'].shift(1) == 1) &
                              (df['bull_candle'] == 1)).astype(int)
        df['evening_star'] = ((df['bull_candle'].shift(2) == 1) &
                              (df['doji'].shift(1) == 1) &
                              (df['bear_candle'] == 1)).astype(int)

        # --- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ ---
        df['corr_price_volume_7'] = c.rolling(7).corr(v)
        df['corr_obv_price_7'] = df['obv'].rolling(7).corr(c)

        # --- Volume spike ---
        df['volume_spike'] = (v > 1.5 * v.rolling(14).mean()).astype(int)

        # --- –õ–∞–≥–∏ ---
        lag_cols = ['Close', 'Volume', 'rsi', 'macd', 'macd_signal', 'obv', 'stoch_rsi', 'adx', 'cci', 'williams_r',
                    'momentum', 'cmf']
        for col in lag_cols:
            for lag in range(1, 4):
                df[f'{col}_lag{lag}'] = df[col].shift(lag)

        # --- –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è ---
        df['future_return'] = df['Close'].shift(-future_horizon) / df['Close'] - 1
        df['target'] = (df['future_return'] > threshold).astype(int)

        # --- –°–≤–µ—á–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ---
        candle_features = pd.DataFrame({
            'body': abs(c - o),
            'upper_shadow': h - np.maximum(c, o),
            'lower_shadow': np.maximum(0, np.minimum(c, o) - l)
        }).replace([np.inf, -np.inf], 0).fillna(0)
        candle_scaled = StandardScaler().fit_transform(candle_features)
        kmeans = KMeans(n_clusters=6, random_state=42).fit(candle_scaled)
        df['candle_cluster'] = kmeans.labels_

        # --- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª ---
        signals = ['sma_signal', 'ema_crossover', 'rsi_signal', 'macd_signal_bin', 'volatility_signal']
        df['combined_signal'] = df[signals].sum(axis=1)

        # --- –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ---
        df.fillna(method='ffill', inplace=True)
        df.fillna(method='bfill', inplace=True)

        return df

    # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
    TRANSFORMER_PATH = "transformer_binary_15min.pt"
    SCALER_PATH = "scaler_ttf.save"
    TRANSFORMER_LOG = "btc_model_predictions.csv"
    RL_LOG = "btc_rl_inference_log_v2.csv"
    RL_MODEL_PATH = "ppo_btc_trading_v18"

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
    df_new = get_btc_and_append_csv()
    df_indicators = compute_indicators_v6(df_new, kalman_smooth=True)

    df_model = df_indicators.drop(columns=["future_return", "High", "Close", "Low", "Open"]).copy()
    df_model = df_model.replace([np.inf, -np.inf], np.nan).dropna()
    scaler = joblib.load(SCALER_PATH)

    X = df_model.drop(columns=["target"])
    X_scaled = scaler.transform(X)
    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(0)  # (1, seq_len, features)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X_tensor = X_tensor.to(device)

    # --- Timestamp (—Å–ª–µ–¥—É—é—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª) ---
    last_timestamp = df_new.index[-1]
    prediction_timestamp = pd.to_datetime(last_timestamp) + pd.Timedelta(minutes=15)

    # --- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏–Ω—Ñ–µ—Ä–µ–Ω—Å ---
    model = TransformerBinaryClassifier(input_size=X_scaled.shape[1])
    model.load_state_dict(torch.load(TRANSFORMER_PATH, map_location=device))
    model.to(device)
    model.eval()

    with torch.no_grad():
        output = model(X_tensor)  # –û–∂–∏–¥–∞–µ—Ç—Å—è shape (batch_size=1, seq_len=..., 1) –∏–ª–∏ (1, 1) ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        # –ï—Å–ª–∏ output shape (1, 1, 1), —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è
        if output.dim() == 3:
            output = output.squeeze(0).squeeze(-1)  # (seq_len,) –∏–ª–∏ (1,)
        elif output.dim() == 2:
            output = output.squeeze(-1)  # (seq_len,) –∏–ª–∏ (batch_size,)

        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π timestep (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª)
        last_pred_logit = output[-1]  # –µ—Å–ª–∏ seq_len > 1, –∏–ª–∏ output[0] –µ—Å–ª–∏ –æ–¥–∏–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —à–∞–≥

        prob = torch.sigmoid(last_pred_logit).item()
        transformer_pred = int(prob > 0.5)

    # --- RL –∏–Ω—Ñ–µ—Ä–µ–Ω—Å ---
    df_model_rl = df_indicators.drop(columns=["future_return"]).copy()
    df_model_rl = df_model_rl.replace([np.inf, -np.inf], np.nan).dropna()


    excluded = ["future_return", "target"]
    state_columns = [col for col in df_model_rl.columns if col not in excluded]
    env = BTCTradingEnv(df_model_rl, state_columns)
    obs, _ = env.reset()
    rl_model = PPO.load(RL_MODEL_PATH, env=env, custom_objects={"policy_class": TransformerPolicy})
    action, _ = rl_model.predict(obs, deterministic=True)
    obs_before = obs.copy()
    try:
        obs, reward, done, _, info = env.step(action)
        next_obs = obs.copy()
    except IndexError:
        print("‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü df, step() –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        reward = 0.0
        done = True
        info = {"error": "IndexError on step()"}
        next_obs = obs_before.copy()

    # --- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç—Ä–æ–∫–∏ (–¥–ª—è –æ–±–æ–∏—Ö –ª–æ–≥–æ–≤) ---
    def update_previous_row(csv_path, close_price, open_price):
        try:
            log_df = pd.read_csv(csv_path, parse_dates=["timestamp"])
        except FileNotFoundError:
            return

        if len(log_df) == 0:
            return

        real_class = int(close_price > open_price)
        log_df.at[log_df.index[-1], "real_price"] = close_price
        log_df.at[log_df.index[-1], "real_class"] = real_class
        log_df.to_csv(csv_path, index=False)

    download_csv_old('1aca3KCgbVKzAqGrNMRzcI5NClfBM5QYM', 'btc_model_predictions.csv')
    update_previous_row(TRANSFORMER_LOG, df_model_rl["Close"].iloc[-1], df_model_rl["Open"].iloc[-1])



    # --- –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ ---
    transformer_row = {
        "timestamp": prediction_timestamp,
        "prediction": transformer_pred,
        "probability": prob,
        "real_price": None,
        "real_class": None
    }
    try:
        pred_df = pd.read_csv(TRANSFORMER_LOG, parse_dates=["timestamp"])
    except FileNotFoundError:
        pred_df = pd.DataFrame(columns=transformer_row.keys())
    pred_df = pd.concat([pred_df, pd.DataFrame([transformer_row])], ignore_index=True)
    pred_df.to_csv(TRANSFORMER_LOG, index=False)



    # === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –æ–±–Ω–æ–≤–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–≤—É—Ö—Å—Ç—Ä–æ—á–Ω—ã–π –ª–æ–≥ ===
    def update_rl_log_tail_to_drive(log_path: str, new_row_dict: dict):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –ª–æ–≥-—Ñ–∞–π–ª, –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —Ä—è–¥–æ–º —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π.
        –ó–∞—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –Ω–æ–≤—ã–π —Ñ–∞–π–ª —Å —Ç–µ–∫—É—â–∏–º –≤—Ä–µ–º–µ–Ω–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞ Google Drive.
        """

        if not os.path.exists(log_path):
            print(f"[–û—à–∏–±–∫–∞] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {log_path}")
            return

        try:
            rl_df = pd.read_csv(log_path)

            if rl_df.empty:
                print(f"[–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ] –õ–æ–≥ –ø—É—Å—Ç–æ–π. –î–æ–±–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.")
                df_tail = pd.DataFrame([new_row_dict])
            else:
                last_row = rl_df.iloc[-1].to_dict()
                df_tail = pd.DataFrame([last_row, new_row_dict])

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º
            timestamp_str = datetime.now().strftime("%Y-%m-%d_%H-%M")
            temp_log_path = f"btc_rl_tail_{timestamp_str}.csv"
            df_tail.to_csv(temp_log_path, index=False)
            print(f"[‚úì] –õ–æ–≥ –æ–±–Ω–æ–≤–ª—ë–Ω: {temp_log_path}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ Google Drive
            upload_csv_to_folder(temp_log_path, FOLDER_ID)

        except Exception as e:
            print(f"[–û—à–∏–±–∫–∞] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –ª–æ–≥: {e}")


    # new_row = {
    #     "timestamp": prediction_timestamp,
    #     "action": int(action),
    #     "reward": float(reward),
    #     "done": done,
    #     "obs": obs_before.tolist(),
    #     "next_obs": next_obs.tolist(),
    #     "real_price": None,
    #     "entry_price": info.get("entry_price", None),
    #     "unrealized_profit": info.get("unrealized_profit", None),
    #     "position_before": info.get("position", 0),
    #     "close_price": df_model_rl["Close"].iloc[-1],
    #     "open_price": df_model_rl["Open"].iloc[-1],
    # }
    #
    # latest_log_path = get_latest_tail_log()
    # update_rl_log_tail_to_drive(latest_log_path, new_row)

    new_row = {
        "timestamp": prediction_timestamp.isoformat(),  # –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π
        "action": int(action),
        "reward": float(reward),
        "done": done,
        "obs": obs_before.tolist(),
        "next_obs": next_obs.tolist(),
        "real_price": None,  # –ø–æ–∫–∞ –Ω–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        "entry_price": info.get("entry_price", None),
        "unrealized_profit": info.get("unrealized_profit", None),
        "position_before": info.get("position", 0),
        "close_price": df_model_rl["Close"].iloc[-1],
        "open_price": df_model_rl["Open"].iloc[-1],
        "real_class": None
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
    update_log_json_and_upload(service, new_row)

    real_close_price = df_indicators["Close"].iloc[-1]
    real_open_price = df_indicators["Open"].iloc[-1]

    # –ß–µ—Ä–µ–∑ 15 –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
    real_update = {
        "timestamp": new_row["timestamp"],  # —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ timestamp, —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å
        "real_price": real_close_price,
        "real_class": int(real_close_price > real_open_price),
    }

    update_log_json_and_upload(service, real_update)

    print(f"[‚úì] –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à—ë–Ω. Prediction Time: {prediction_timestamp}")

if __name__ == '__main__':
    print(get_prediction_report())